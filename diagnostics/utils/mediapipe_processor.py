# diagnostics/utils/mediapipe_processor.py
import cv2
import numpy as np
import mediapipe as mp
import os
import logging
from datetime import datetime
from django.conf import settings
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe.framework.formats import landmark_pb2

mp_drawing = mp.solutions.drawing_utils
mp_pose = mp.solutions.pose
logger = logging.getLogger(__name__)

MODEL_PATH = os.path.join(settings.BASE_DIR, "assets", "pose_landmarker_full.task")

def process_video_with_mediapipe(video_path: str, job_type: str = "GENERAL", calibration_factor: float = 1.0):
    """
    Feldolgozza a vide√≥t MediaPipe PoseLandmarker seg√≠ts√©g√©vel.
    Annot√°lt (eredeti + skeleton) MP4 vide√≥ + kulcspont adatok.
    """
    # ‚úÖ Modell ellen≈ërz√©s
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"‚ùå Hi√°nyzik a modell: {MODEL_PATH}")
    logger.info(f"‚úÖ Modell bet√∂ltve: {MODEL_PATH}")

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Nem siker√ºlt megnyitni a vide√≥t: {video_path}")

    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    logger.info(f"üìπ Vide√≥ info: {width}x{height}, {fps} FPS, {total_frames} frame")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join("/tmp", f"mediapipe_output_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)

    skeleton_video_path = os.path.join(output_dir, "skeleton_video.mp4")
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(skeleton_video_path, fourcc, fps, (width, height))

    BaseOptions = python.BaseOptions
    PoseLandmarkerOptions = vision.PoseLandmarkerOptions
    VisionRunningMode = vision.RunningMode

    # ‚¨áÔ∏è CS√ñKKENTETT THRESHOLD-OK
    options = PoseLandmarkerOptions(
        base_options=BaseOptions(model_asset_path=MODEL_PATH),
        running_mode=VisionRunningMode.VIDEO,
        min_pose_detection_confidence=0.3,  # Cs√∂kkentve 0.5-r≈ël
        min_pose_presence_confidence=0.3,   # Cs√∂kkentve 0.5-r≈ël
        min_tracking_confidence=0.3,        # Cs√∂kkentve 0.5-r≈ël
        output_segmentation_masks=False,
    )
    landmarker = vision.PoseLandmarker.create_from_options(options)
    logger.info("‚úÖ MediaPipe PoseLandmarker inicializ√°lva.")

    frame_number = 0
    raw_keypoints = []
    keyframes = []
    detected_frames = 0  # üÜï Detekt√°lt frame-ek sz√°ml√°l√≥ja

    while cap.isOpened():
        success, image = cap.read()
        if not success:
            break

        # ‚úÖ Frame valid√°ci√≥
        if image is None or image.size == 0:
            logger.error(f"‚ùå Frame {frame_number} √ºres, √°tugr√°s!")
            frame_number += 1
            continue

        timestamp_ms = int(frame_number * 1000 / fps)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)

        results = landmarker.detect_for_video(mp_image, timestamp_ms)
        annotated_image = image.copy()

        # üÜï DEBUG: Pose detekt√°l√°s ellen≈ërz√©se
        if results.pose_landmarks:
            detected_frames += 1
            if frame_number % 30 == 0:  # Csak minden 30. frame-n√©l logolunk
                logger.info(f"‚úÖ Frame {frame_number}/{total_frames}: {len(results.pose_landmarks[0])} landmark OK")
            
            drawing_spec_landmark = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)
            drawing_spec_connection = mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)

            landmark_list = results.pose_landmarks[0]
            mp_landmark_list = landmark_pb2.NormalizedLandmarkList()

            frame_keypoints = []
            frame_world_landmarks = []

            for lm in landmark_list:
                l = mp_landmark_list.landmark.add()
                # üí° FIGYELEM! A LandmarkList-be tov√°bbra is a normaliz√°lt √©rt√©kek mennek (vagy kihagyjuk a felt√∂lt√©st), 
                # de a raw_keypoints-ba m√°r a SK√ÅL√ÅZOTT!
                # Itt hagyjuk a normaliz√°lt √©rt√©keket, mert a rajzol√°shoz is az kell.

                # üü¢ A raw_keypoints-ba SK√ÅL√ÅZOTT √©rt√©ket tesz√ºnk!
                scaled_x = lm.x * calibration_factor
                scaled_y = lm.y * calibration_factor
                scaled_z = lm.z * calibration_factor # Z koordin√°ta sk√°l√°z√°sa is, ha haszn√°lva van

                frame_keypoints.append(
                    {"x": scaled_x, "y": scaled_y, "z": scaled_z, "v": getattr(lm, "visibility", 1.0)}
                )

            if results.pose_world_landmarks:
                for wlm in results.pose_world_landmarks[0]:
                    frame_world_landmarks.append(
                        {"x": wlm.x, "y": wlm.y, "z": wlm.z, "v": getattr(wlm, "visibility", 1.0)}
                    )

            # ‚úÖ RAJZOL√ÅS
            mp_drawing.draw_landmarks(
                annotated_image,
                mp_landmark_list,
                mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=drawing_spec_landmark,
                connection_drawing_spec=drawing_spec_connection,
            )

            raw_keypoints.append(frame_keypoints)
            keyframes.append({
                "frame": frame_number,
                "time_ms": timestamp_ms,
                "keypoints": frame_keypoints,
                "world_landmarks": frame_world_landmarks,
                "frame_image": annotated_image if frame_number == total_frames // 2 else None,
            })
        else:
            # ‚ö†Ô∏è Pose NEM detekt√°lva
            if frame_number % 30 == 0:
                logger.warning(f"‚ö†Ô∏è Frame {frame_number}/{total_frames}: NINCS pose landmark!")

        out.write(annotated_image)
        frame_number += 1

    cap.release()
    out.release()
    landmarker.close()

    # üÜï √ñSSZEFOGLAL√ì
    detection_rate = (detected_frames / frame_number * 100) if frame_number > 0 else 0
    logger.info(f"üéØ {frame_number} frame feldolgozva")
    logger.info(f"‚úÖ {detected_frames} frame-ben detekt√°lva pose ({detection_rate:.1f}%)")
    logger.info(f"üìπ Annot√°lt vide√≥: {skeleton_video_path}")

    # ‚ö†Ô∏è Ha t√∫l kev√©s detekt√°l√°s volt, figyelmeztet√©s
    if detection_rate < 10:
        logger.error(f"‚ùå KRITIKUS: Csak {detection_rate:.1f}% frame-ben detekt√°lva pose!")
        logger.error("üí° Ellen≈ërizd: vide√≥ min≈ës√©g, vil√°g√≠t√°s, kamera t√°vols√°g, modell f√°jl")

    return raw_keypoints, skeleton_video_path, keyframes

def process_image_with_mediapipe(image_path: str):
    """
    Statikus k√©pet dolgoz fel MediaPipe PoseLandmarker seg√≠ts√©g√©vel.
    Ha a MediaPipe nem tal√°l embert, MoveNet fallback ker√ºl alkalmaz√°sra.
    """
    import mediapipe as mp
    from mediapipe.tasks import python as mp_python
    from mediapipe.tasks.python import vision as mp_vision
    import tensorflow as tf
    import tensorflow_hub as hub

    os.environ["MEDIAPIPE_USE_GPU"] = "false"
    logger.info("‚öôÔ∏è MediaPipe CPU m√≥dra √°ll√≠tva.")

    if not os.path.exists(image_path):
        raise FileNotFoundError(f"‚ùå A k√©p nem tal√°lhat√≥: {image_path}")

    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"‚ùå Modellf√°jl hi√°nyzik: {MODEL_PATH}")

    # --- El≈ëk√©sz√≠t√©s: orient√°ci√≥, kontraszt jav√≠t√°s ---
    image_bgr = cv2.imread(image_path)
    if image_bgr is None or image_bgr.size == 0:
        raise ValueError(f"Nem siker√ºlt beolvasni a k√©pet: {image_path}")

    h, w = image_bgr.shape[:2]
    if w > h:
        image_bgr = cv2.rotate(image_bgr, cv2.ROTATE_90_CLOCKWISE)
        logger.info("üåÄ K√©p elforgatva √°ll√≥ orient√°ci√≥ra.")

    # Kontraszt jav√≠t√°s
    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l2 = clahe.apply(l)
    lab = cv2.merge((l2, a, b))
    image_bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

    temp_preprocessed_path = image_path.replace(".jpg", "_preprocessed.jpg")
    cv2.imwrite(temp_preprocessed_path, image_bgr)
    logger.info(f"‚úÖ El≈ëk√©sz√≠tett k√©p mentve: {temp_preprocessed_path}")

    # --- MediaPipe be√°ll√≠t√°sok ---
    base_options = mp_python.BaseOptions(model_asset_path=MODEL_PATH)
    options = mp_vision.PoseLandmarkerOptions(
        base_options=base_options,
        output_segmentation_masks=False,
        running_mode=mp_vision.RunningMode.IMAGE,
        min_pose_detection_confidence=0.3,
        min_pose_presence_confidence=0.3,
    )

    result = None

    try:
        with mp_vision.PoseLandmarker.create_from_options(options) as landmarker:
            mp_image = mp.Image.create_from_file(temp_preprocessed_path)
            result = landmarker.detect(mp_image)

            if result.pose_landmarks:
                logger.info(f"‚úÖ MediaPipe tal√°lt {len(result.pose_landmarks[0])} landmark pontot.")
            else:
                logger.warning("‚ö†Ô∏è MediaPipe nem tal√°lt alakot (els≈ë pr√≥b√°lkoz√°s).")

        # --- M√°sodik pr√≥b√°lkoz√°s enged√©kenyebb be√°ll√≠t√°sokkal ---
        if not result.pose_landmarks or not result.pose_world_landmarks:
            logger.warning("üîÅ √öjrapr√≥b√°l√°s laz√°bb k√ºsz√∂b√∂kkel...")
            options.min_pose_detection_confidence = 0.15
            options.min_pose_presence_confidence = 0.15
            with mp_vision.PoseLandmarker.create_from_options(options) as landmarker2:
                result = landmarker2.detect(mp_image)

        # --- Selfie m√≥d pr√≥b√°lkoz√°s ---
        if not result.pose_landmarks:
            logger.warning("üîÅ √öjrapr√≥b√°l√°s SELFIE m√≥dban...")
            selfie_options = mp_vision.PoseLandmarkerOptions(
                base_options=base_options,
                running_mode=mp_vision.RunningMode.IMAGE,
                output_segmentation_masks=False,
                num_poses=1,
                min_pose_detection_confidence=0.15,
                min_pose_presence_confidence=0.15,
            )
            with mp_vision.PoseLandmarker.create_from_options(selfie_options) as selfie_landmarker:
                result = selfie_landmarker.detect(mp_image)

        # --- Ha MediaPipe nem tal√°l, j√∂n a MoveNet fallback ---
        if not result or not result.pose_landmarks or not result.pose_world_landmarks:
            logger.warning("‚ö†Ô∏è MediaPipe nem tal√°lt alakot ‚Äî √°tv√°lt√°s MoveNet fallback-re...")
            try:
                import tensorflow as tf
                import tensorflow_hub as hub

                movenet_model = hub.load("https://tfhub.dev/google/movenet/singlepose/thunder/4")
                movenet = movenet_model.signatures['serving_default']

                # K√©p el≈ëfeldolgoz√°sa (RGB √©s √°tm√©retez√©s)
                image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
                input_image = tf.image.resize_with_pad(tf.expand_dims(image_rgb, axis=0), 256, 256)
                input_image = tf.cast(input_image, dtype=tf.int32)

                outputs = movenet(input_image)
                keypoints = outputs['output_0'].numpy()[0, 0, :, :]

                normalized_landmarks = []
                for idx, (y, x, c) in enumerate(keypoints):
                    normalized_landmarks.append({
                        "x": float(x),
                        "y": float(y),
                        "z": 0.0,
                        "v": float(c)
                    })

                world_landmarks = [
                    {"id": i, "x": lm["x"], "y": lm["y"], "z": 0.0, "v": lm["v"]}
                    for i, lm in enumerate(normalized_landmarks)
                ]

                # Annot√°lt k√©p ment√©se MoveNettel
                annotated_path = image_path.replace(".jpg", "_annotated_movenet.jpg")
                annotated = image_bgr.copy()
                for lm in normalized_landmarks:
                    cx, cy = int(lm["x"] * annotated.shape[1]), int(lm["y"] * annotated.shape[0])
                    cv2.circle(annotated, (cx, cy), 3, (0, 255, 0), -1)
                cv2.imwrite(annotated_path, annotated)
                logger.info(f"‚úÖ MoveNet fallback sikeres ({len(normalized_landmarks)} pont).")
                logger.info(f"üì∏ MoveNet annot√°lt k√©p mentve: {annotated_path}")

                return {"world_landmarks": world_landmarks, "normalized_landmarks": normalized_landmarks}

            except Exception as e:
                logger.error(f"‚ùå MoveNet fallback hiba: {e}", exc_info=True)
                raise ValueError("Sem a MediaPipe, sem a MoveNet nem tal√°lt embert a k√©pen!")

        # --- MediaPipe eredm√©ny feldolgoz√°sa ---
        world_landmarks = [
            {"id": i, "x": lm.x, "y": lm.y, "z": lm.z, "v": getattr(lm, "visibility", 1.0)}
            for i, lm in enumerate(result.pose_world_landmarks[0])
        ]
        normalized_landmarks = [
            {"x": lm.x, "y": lm.y, "z": lm.z, "v": getattr(lm, "visibility", 1.0)}
            for lm in result.pose_landmarks[0]
        ]

        annotated_path = image_path.replace(".jpg", "_annotated.jpg")
        annotated_image = image_bgr.copy()
        mp_drawing = mp.solutions.drawing_utils
        mp_pose = mp.solutions.pose
        mp_drawing.draw_landmarks(
            annotated_image,
            result.pose_landmarks[0],
            mp_pose.POSE_CONNECTIONS,
            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=2),
            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=2),
        )
        cv2.imwrite(annotated_path, annotated_image)
        logger.info(f"üì∏ Annot√°lt k√©p mentve: {annotated_path}")

        return {"world_landmarks": world_landmarks, "normalized_landmarks": normalized_landmarks}

    except Exception as e:
        logger.error(f"‚ùå process_image_with_mediapipe hiba: {e}", exc_info=True)
        raise